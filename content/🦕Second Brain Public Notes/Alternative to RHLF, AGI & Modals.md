---
tags:
  - computer-science/ai
---

RLHF typically refers to "Reinforcement Learning with Human Feedback". Reinforcement Learning (RL) is a type of machine learning that involves training an agent to make decisions based on feedback from its environment. In RLHF, the agent also receives feedback from humans in the form of ratings or evaluations of its actions, which can help it learn more quickly and accurately.

RLHF is an active research area in artificial intelligence, with applications in fields such as robotics, gaming, and personalized recommendation systems. It seeks to address the challenges of RL in scenarios where the agent has limited access to feedback from the environment and requires human input to improve its performance.

Reinforcement Learning with Human Feedback (RLHF) is a rapidly developing area of research in artificial intelligence, and there are several advanced techniques that have been developed to improve the performance of RLHF systems. Here are some examples:

- `Inverse Reinforcement Learning (IRL)`: IRL is a technique that allows the agent to learn a reward function from human feedback, rather than relying on pre-defined reward functions. This makes it possible for the agent to learn from more complex feedback signals, such as demonstrations of desired behavior.
    
- `Apprenticeship Learning`: Apprenticeship learning is a technique that combines IRL with supervised learning to enable the agent to learn from both human feedback and expert demonstrations. This can help the agent learn more quickly and effectively, as it is able to learn from both positive and negative feedback.
    
- `Interactive Machine Learning (IML)`: IML is a technique that involves active interaction between the agent and the human expert, allowing the expert to provide feedback on the agent's actions in real-time. This can help the agent learn more quickly and efficiently, as it can receive feedback on its actions at each step of the learning process.
    
- `Human-in-the-Loop Reinforcement Learning (HITLRL)`: HITLRL is a technique that involves integrating human feedback into the RL process at multiple levels, such as reward shaping, action selection, and policy optimization. This can help to improve the efficiency and effectiveness of the RLHF system by taking advantage of the strengths of both humans and machines.
    

Here are some examples of Reinforcement Learning with Human Feedback (RLHF):

- `Game Playing`: In game playing, human feedback can help the agent learn strategies and tactics that are effective in different game scenarios. For example, in the popular game of Go, human experts can provide feedback to the agent on its moves, helping it improve its gameplay and decision-making.
    
- `Personalized Recommendation Systems`: In recommendation systems, human feedback can help the agent learn the preferences of individual users, making it possible to provide personalized recommendations. For example, the agent could use feedback from users on recommended products to learn which features are most important to them.
    
- `Robotics`: In robotics, human feedback can help the agent learn how to interact with the physical environment in a safe and efficient manner. For example, a robot could learn to navigate a new environment more quickly with feedback from a human operator on the best path to take or which objects to avoid.
    
- `Education`: In education, human feedback can help the agent learn how to teach students more effectively. For example, an AI-based tutor could use feedback from teachers on which teaching strategies work best with different students, helping to personalize the learning experience.

Further, people often ask if universities — which don't have the massive compute resources of big tech — can still do cutting-edge research on large language models (LLMs). 

The answer, to me, is obviously yes! This article is a beautiful example of algorithmic and mathematical insight arrived at by an academic group thinking deeply._ 

**RLHF** became a key algorithm for LLM training . A typical implementation of the algorithm works as follows: 
- _Get humans to compare pairs of LLM outputs, generated in response to the same prompt, to specify which one they prefer. For example, humans typically prefer the more helpful, less toxic output._
- _Use the human preferences to learn a reward function. The reward function, typically represented using a transformer network, is trained to give a higher reward (or score) to the outputs that the humans preferred._
- _Finally, using the learned reward, run a reinforcement learning algorithm to tune the LLM to (i) maximize the reward of the answers generated, while (ii) not letting the LLM change too much (as a form of regularization)._

_This is a relatively complex algorithm. It needs to separately represent a reward function and an LLM. Also, the final, reinforcement learning step is well known to be finicky to the choice of hyperparameters._

[RLHF for LLMs](#)  
![](https://i.imgur.com/7IQsYsv.png)


![Screenshot 2024-01-09 at 5.04.13 PM](https://ci3.googleusercontent.com/meips/ADKq_NZu_tqcf5d_yFm9YVmPIfhmStisD7hnVn-ar1RxzryNYGDWASJkPmQxE3O98sERDcgT6VIQC7Ha3wK5Y8B9GN6g0ie--nnlSBaxnDh_WuLVqFQs1kVZmLyDSe2OFmDvxcELZLt1cUEmCeeJhkDQ__7kkOaXPi73b8S9nxYD3sShaB-e_gOrspGgAWJoJ47ZpG1cEKbCyFd1N3RIfEiKOaXvYgr8NvKafLm0H4envhUMXCvxaNOGM4a_yDri=s0-d-e1-ft#https://info.deeplearning.ai/hs-fs/hubfs/Screenshot%202024-01-09%20at%205.04.13%20PM.png?width=1200&upscale=true&name=Screenshot%202024-01-09%20at%205.04.13%20PM.png)

_DPO dramatically simplifies the whole thing. Rather than needing separate transformer networks to represent a reward function and an LLM, the authors show how, given an LLM, you can figure out the reward function (plus regularization term) that that LLM is best at maximizing. This collapses the two transformer networks into one. Thus, you now need to train only the LLM and no longer have to deal with a separately trained reward function. The DPO algorithm trains the LLM directly, so as to make the reward function (which is implicitly defined by the LLM) consistent with the human preferences. Further, the authors show that DPO is better at achieving RLHF's optimization objective (that is, (i) and (ii) above) than most implementations of RLHF itself._ 

_RLHF is a key building block of the most advanced LLMs. It’s fantastic that these Stanford authors — through clever thinking and mathematical insight — seem to have replaced it with something simpler and more elegant. While it's easy to get excited about a piece of research before it has stood the test of time, I am cautiously optimistic that DPO will have a huge impact on LLMs and beyond in the next few years. Indeed, it is already making its way into some top-performing models, such as Mistral’s [Mixtral](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWdlVJ2qRbgRW1C0m9M49jnZJW2XSKgS587CH1N28qGwC3qgyTW7lCdLW6lZ3lLW63xs-Q4h-B_SW82b99V3Mpw_zW6C5X588DLQCqW5jGlvC6gGq6BW2GlQpc6Zg2JMW6B5yTR5KDhJTW8tYFjR3cxcyFW3np0XQ1fnlk6V_Ff7438-ZlsW4qCT-q13_M9QW2S6-923hhDX_W1N7-5y7ZCXLhW2zQF5r6vP6gpW22FT2c4z_68fW6p210z7rvlf-MnRXwSygnstW91HsMM8N1qt4W6qWTb88gj3p-W513_b998wrbCW3xxxVQ8nht_GW4cMKwJ2lm94pW8VH2sT5Yx9_DW1pG2xT6pkZ8fW7Zp4jH6Hyzdrf8GLRxj04)._  

_That we can replace such fundamental building blocks of LLMs is a sign that the field is still new and much innovation lies ahead. Also, while it's always nice to have massive numbers of NVIDIA H100 or AMD MI300X GPUs, this work is another illustration — out of many, I want to emphasize — that deep thinking with only modest computational resources can carry you far._ 

_  
A few weeks ago at NeurIPS (where DPO was published), I found it remarkable both (i) how much highly innovative research there is coming out of academic labs, independent labs, and companies small and large, and (ii) how much our media landscape skews attention toward work published by the big tech companies. I suspect that if DPO had been published by one of the big LLM companies, it would have made a huge PR splash and been announced as a massive breakthrough. Let us all, as builders of AI systems, make sure we recognize the breakthroughs wherever they occur._

_Keep learning!_

_Andrew Ng_

## AGI Defined

How will we know if someone succeeds in building artificial general intelligence (AGI)? A recent paper defines milestones on the road from calculator to superintelligence.

What’s new: Researchers at Google led by Meredith Ringel Morris [propose](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWdlVJ2qRbgRW1C0m9M49jnZJW2XSKgS587CH1N28qGwj3qgyTW6N1vHY6lZ3ngW8Pd7PG28gt7QVrdYSp2987wRW7FglKz6dQ3s0W7wsq-H6t1jtVW1rbC7F7SdfQ6W6R3dNl1q94WSW35H-2y407jCFW29Lfrl39Z5rmW8ZVPTc6pPf_xW4Gylzk5Z-h3yW1_sw8S3RPnbQW6QRjLN5yn_Y9W25f6mm3lpNqNW5jLpTv7SDVn8W1ldYWS8jYCq6W1bwhTC1_bJ3WW3x71J-5TSY2RW2H1FSx1nlJBkW6f-nHm6PcmtqN8DtfHMNQq4VW117Thp4Wpg5cW6_mRr47Zp3rmf2YPpg804) a taxonomy of AI systems according to their degree of generality and ability to perform cognitive tasks. They consider today’s large multimodal models to be “emerging AGI.”

AGI basics: Artificial general intelligence is commonly defined as AI that can perform any intellectual task a human can. Shane Legg (who co-founded DeepMind) and Ben Goertzel (co-founder and CEO of [SingularityNet](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWdlVJ2qRbgRW1C0m9M49jnZJW2XSKgS587CH1N28qGwj3qgyTW6N1vHY6lZ3p_N61j43XntFnvW210P9K4tJp2rW4yLmb02K4ds5W4hsP0v5G3m7_W6w5BVm9cP6r6W4vwL608ccnyDW9dWRGG6bvXCsN60zFRvr6H0LW6FGNJt2tX-ZJW2JbNwX4MrqlgW4LYvF71X-nSHV-Cn7m8bN_VCN3sMl54pWXhkVhRjgh62BXK4W9bm_cd5tRFwvW78JJB86yfTTlW57p7_D1QxB0HW3cR7fT6K9dfpW99VGRS3jv-TCW5VWbDh3XHLwCW3p9pxB5_KMCLW97RM7k57lQwBf6FNBvW04)) coined the term AGI for a 2007 collection of [essays](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWdlVJ2qRbgRW1C0m9M49jnZJW2XSKgS587CH1N28qGwC3qgyTW7lCdLW6lZ3mXW1HLXn577h_pdW525Csh16tWymW5BZDN41dN0mRW1CzL-h1R459KW1cS75_7km-KzW4QtQKT11-tjpW9kPtc549YNgQW52jGDZ2JBjF2VKfMXN71rRNkW4PRtBV8BPhM4W8RxJFH9lwvw9W8ZVMYp413l__W44T5C24y3SS6W7QQpyF8Pp9WtN84BfWn8qLC_W5J-SjN6_jDMzW5H6nT43RN6QDW6-d0Qk151T_ZN1VBP7ZYkV0TW4w_kv06bmDYkW2M_nqT1C-NkGW5FYrP-5lqvBcW5RlFm-96ML2CW9jyG7q8wsgy5f10mh4H04). Subsequently, companies like DeepMind and OpenAI, which explicitly aim to develop AGI, propelled the idea into the mainstream.

How it works: The taxonomy categorizes systems as possessing narrow skills (not AGI) or general capabilities (AGI). It divides both narrow and general systems into five levels of performance beyond calculator-grade Level 0. It also includes a metric for degree of autonomy.

- Narrow systems perform one distinct task; they may perform at one of the five levels, but they are not AGI. General systems perform a range of tasks (which the authors don’t specify) that align with real-world activities of broad value to people, including but not limited to linguistic, mathematical, logical, spatial reasoning, social, learning, and creative tasks. Crucially, they can learn how to learn new skills and when to ask humans for more information. The authors classify general systems as AGI at various levels of performance.
- Level 1 (“emerging”) matches or slightly exceeds unskilled humans. Levels 2 (“competent”), 3 (“expert”), and 4 (“virtuoso”) systems surpass the 50th, 90th and 99th percentiles of skilled human performance, respectively. Level 5 (“superhuman” or “artificial superintelligence”) outperforms 100 percent of skilled humans.
- Most current systems that perform at Level 2 or higher are narrow. For example, [AlphaFold](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWdlVJ2qRbgRW1C0m9M49jnZJW2XSKgS587CH1N28qGwW3qgyTW7Y8-PT6lZ3ppW5wPqN47hNkh_W5RP0T082qsdzW5DH4452x6z6FW2MjdjV6dbF71N3c38WXNXpZPW4jCRgl6bP3FgW2dQ2Sj81vWdDW5pkDb361hWzhW8CTykK7652BbW75tKRy6D6q0VW15W6GQ7SMWYGW629bF18zQ79wW819wPC4Qn29hW3lzmln6zK4g-W22R4JP5t-1tbW7mMtPJ7WLkKtW3ywP0-51Cyl1N32DV_dTdJK3W9cJcrf8wjpRTW7Vv0328hm_XRW9cfKXN5msm7XN1G5Mn7z_1xKW7YHzZc6-49bCW3ZMNmG47FYbGW6C8PRB88NMnTW9jDSbY4cpDK2f2wj6w604), which finds the shapes of protein molecules, achieves Level 5 performance but only in a single task. On the other hand, the authors consider large language models like Bard, ChatGPT, and Lama 2 to be general systems at Level 1 (although their performance may achieve Level 2 in some tasks). 
- The authors’ autonomy scale ranges from tools for which humans control the task while the system automates subtasks (the first level of autonomy) to agents that act independently (the fifth). Higher levels of performance can unlock higher levels of autonomy. For instance, Level 4 AGI may be necessary to enable fully autonomous vehicles that are safe and trustworthy.

Yes, but: The authors’ definition identifies some classes of tasks that contribute to generality, but it includes neither a list of tasks a system must perform to be considered general nor a method for selecting them. Rather, the authors call on the research community to develop a “living benchmark” for generality that includes a mechanism for adding novel tasks.

Why it matters: AGI is one of the tech world’s hottest buzzwords, yet it has had no clear definition, and various organizations propose different definitions. This lack of specificity makes it hard to talk about related technology, regulation, and other topics. The authors’ framework, on the other hand, supports a more nuanced discussion of the path toward AGI. And it may have high-stakes business implications: Under the terms of their partnership, OpenAI can withhold from Microsoft models that attain AGI. Applying the authors’ taxonomy would make it harder for one of the parties to move the goalposts. 

We’re thinking: Defining AGI is tricky! For instance, OpenAI defines AGI as “a highly autonomous system that outperforms humans at most economically valuable work.” This definition, had it been formulated in the early 1900s, when agriculture accounted for 70 percent of work globally, would have described the internal combustion engine.

![GILL](https://ci3.googleusercontent.com/meips/ADKq_NY_5fQ2L4oEFvxicuXkbyqLV_U2AAygc4AYR3LyBjPGRzj70P53T9WbH7FT7lO_LWbH7xr-kwIa4B4hDh42z_oKpvDhMA7LY6bspqQ-3eEcFfmeNHVYaydtOljGlwx_3SwNYjZ2tqMv50w5v0M=s0-d-e1-ft#https://info.deeplearning.ai/hs-fs/hubfs/GILL.gif?width=1200&upscale=true&name=GILL.gif)

## Text or Images, Input or Output

GPT-4V introduced a large multimodal model that generates text from images and, with help from DALL-E 3, generates images from text. However, OpenAI hasn’t fully explained how it built the system. A separate group of researchers described their own method.

What's new: Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov at Carnegie Mellon University proposed [Generating Images with Large Language Models](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWdlVJ2qRbgRW1C0m9M49jnZJW2XSKgS587CH1N28qGwj3qgyTW6N1vHY6lZ3krN2_hqQ7BHBTJW4zTZ2x7F74_HN3dJjFKbpFFWW6R1G-D3py90sN4C9WgCNPf6QW8Gcn4f178pFWW7Myn-t7GZ4tqVm74lT4MLFgqW139sQv5990BsW4clR175h0zslW83XllY4MddjWW3JrmgQ6sFYXnN2LwFYWCvdSrW8RLgcQ1fjh1rW34zh-N23BjswW8_T4yC7xqHl4W91GdmZ7y0Hc3N257nkzZBwLmVbVGvm9gH1H-W88JzFd4ddVLbW8Jgqdm7Wxlk6VR5vc169FMr7f5qKVwF04) (GILL), a training method that enables a large language model and a text-to-image generator to use both text and images as either input or output. Given text and/or image input, it decides whether to retrieve existing images or generate new ones.

Key insight: Models like CLIP and ImageBind map text and image inputs to a similar embedding space, so closely related text and images have similar embeddings. This approach enables a large multimodal model to process both data types. Text outputs, too, can be mapped to the same embedding space, so an image decoder, such as a diffusion model, can use them to produce images or an image retriever to retrieve images.

How it works: The authors used a pretrained [OPT](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWdlVJ2qRbgRW1C0m9M49jnZJW2XSKgS587CH1N28qGwj3qgyTW6N1vHY6lZ3lvW3XxZzg6l4_N2W8GkxHs59dCBkW98ysTM92DBrfW6dH3Vx3hjrDGMRjTkBt3GlFW5t_qt78r3jNjW6NR82w56wJkLN1fQvDH_HXbfW4W7PMY28_ZD6W8HH66g12W-qFW1P-26V79j1GBV10Qgq6r2Y5vW4b6Ndx6hxmnMW39V1gd2mCTVNW3t1JYJ8qRZyDW2q81L92xHbwxV5b9pr7FXYB1W618Rxt61q-qgN7JXcDLvSKMCW8rSR2Z56D8FlW846hCn6R90qnW4Y7W464pDKDpf4JV1zj04) large language model, [ViT-L](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWdlVJ2qRbgRW1C0m9M49jnZJW2XSKgS587CH1N28qGwj3qgyTW6N1vHY6lZ3myW7YQr5-6W0P-ZW5wwR-56VCSQBW1hGCvj89w6VxW4kdYHP2l7--VW8q99Cg4mq-xDW93sM-m84GlCyW9fWVvt1ny2GdW1TgHMm96RpyKVcql316PFrhyW1mwNrK1GgxT7N2znwgpXv8H2W6RZm0s3qGcBSVbZHM887nCKTN86ds69R-g_NW7HGJlb5pc_rQW1_SFXJ9g_zSwW1nmFCJ3vTvp_W7MGrPk4WkKTzW32sTV15r0j9nW34VTK_7SxkdsW6qSKVr7sq-gZN1KLV4ngH6Sqf4qr5-d04) image encoder (taken from CLIP), and pretrained Stable Diffusion text-to-image generator. The authors trained ViT-L to map its embeddings to those produced by OPT. They trained OPT to recognize prompts that request an image and enabled the system to either generate or retrieve images. Finally, a separate linear classifier learned whether to retrieve or generate images. 

- The authors froze the ViT-L, added a linear layer, and trained it as follows: Given an image, the ViT-L-plus-linear-layer produced an image embedding, as usual. Given the image embedding and the first part of the corresponding caption, OPT iteratively tried to predict the next word. The linear layer learned how to modify the embedding so OPT could complete the caption. This enabled OPT to take images as input.
- They added 8 tokens to OPT’s vocabulary and trained the model to emit them at the end of every image caption — a signal that an image should be either retrieved or generated. (Typically a single token is sufficient to denote the end of a caption. However, these tokens corresponded to embeddings that, later, would be used to generate an image, and the authors found that a single token was not sufficiently expressive.)
- Then they enabled Stable Diffusion to produce an image when OPT generated the 8 new tokens. They trained a separate transformer to map OPT’s embeddings associated with the 8 tokens (that is, embeddings produced by the layer before the one that generated the tokens) to those produced by Stable Diffusion’s text encoder.
- Next they enabled the system to retrieve images when OPT generated the 8 tokens. They added linear layers to ViT-L and OPT and trained them to map the ViT-L’s embeddings to the OPT embedding associated with the first token. Specifically, the linear layers learned to minimize the difference between their outputs.
- The authors trained a linear classifier, given the 8 OPT embeddings associated with the tokens, to decide whether to retrieve or generate an image. To build the classifier’s training set, they selected captions from a [collection](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWdlVJ2qRbgRW1C0m9M49jnZJW2XSKgS587CH1N28qGwj3qgyTW6N1vHY6lZ3lGMsR7bPZY0vKW5-9bS15hjf89V-Vvmq4TMZjLW1lK6dr6t7kbDW3qTpqH6_FdBKW8kz-8-5FPfRvVYQZcx1LqJ-mW44kYdp10BKKcW5jPg2T89QBqnW4qTxhn82P-qDW2RKXB_5dqCh2W6vRZ5r78gx6wW8LD7B13x6jT_W2d_QKt8yX1n_W4WfRT76hnKMDW83RYLR7TFNQ_W8sGTJK5yDfZSV8yQ8l8Zg-xwW1wfMmc8b07ZrW1gSq_73bXbGjW6x4sfp7cN1p-N2ZlPSvhdg-Tdrpx7T04) of diverse human-written prompts and, for each one, both generated an image and retrieved the most similar image from CC3M. 5 human judges selected the image that best matched the prompt. This process yielded 900 examples annotated according to whether the image was retrieved or generated.
- At inference, OPT generated tokens and fed the associated embeddings directly to the classifier, which activated the pipeline for either the generation or retrieval.

Results: [VIST](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWdlVJ2qRbgRW1C0m9M49jnZJW2XSKgS587CH1N28qGwj3qgyTW6N1vHY6lZ3mdW5HH4KP58XCnvVHYFJW3Qw1X6W6BKQ9s3snBd1N57VTjPjvFZdVngXXC5tYxYKW2htnmx3KxY3dW5_XhKr86p8TTW1tdLYB4t8HksW25Sr1M7Kr-3ZW2Bg_7-9jkL2MW6Dq98377nLDRW1FvgBw2DyZ74W8ZkBj49dy5WDW2M510K7_0W8TW360hB2567G3dW4xf9BC2t83c8W8lQTmY1RFCxnW6CT75m2--_sXW13ZNwT8TDlZtW5-vPCh2k3j09W4PTbtk5B7Xz8W436Pfh97Nckhf5KpHBT04) is a dataset of 20,000 visual stories, each of which comprises five captioned images. The authors evaluated GILL’s and Stable Diffusion’s abilities, given the final caption or all five captions, to generate the final image in each story based on CLIP similarity scores between generated and ground-truth images. Given one caption, GILL achieved 0.581 similarity and Stable Diffusion achieved 0.592 similarity. Given five captions, GILL achieved 0.612 similarity and Stable Diffusion scored 0.598 similarity, highlighting GILL’s ability to use the context afforded by more extensive input. It did even better (0.641 similarity) given both captions and images, which Stable Diffusion couldn’t handle. The authors also evaluated how well their system retrieved the correct last image from VIST given the 5 captions and the first 4 images. GILL retrieved the correct image 20.3 percent of the time, while their own [FROMAGe](https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWdlVJ2qRbgRW1C0m9M49jnZJW2XSKgS587CH1N28qGwj3qgyTW6N1vHY6lZ3lFN28KK8xNq6yBW37Wp1c3B2lm1W6SVw_L1v1X1zW2RBL9j5YWqlwW1Y26xF6KlknjW16PlmT3ckyK6W6g3-dJ5gKSqBW5hSWFW2fZTLgW2Z4BjR91tpj0W3fwGxX6_jB3KW4lXX434MFFgVW6xSq8G6m8R98W7fJT5y3wDNt-W7lBxbs8WDsZMW8wy8Fm2H-99pW4lZ7Dg1mhm9xW30n75B7RSflvW2KvxqZ4yFq4TW6zjSYB767-7dW1LGQm91-pFxkW29rd9C4Tdz5CVHd4Lk85VZGWdxHtgM04) retrieved the correct image 18.2 percent of the time. In comparison, CLIP, given the 5 captions (without the images), retrieved the correct image 8.8 percent of the time.

Why it matters: Models that wed text and images are advancing rapidly. GILL and other recent models extend single-image input and/or output to any combination of images and text. This capability — which GILL achieves by mapping embeddings of image and text to one another — gives the models more context to generate more appropriate output.

We’re thinking: The authors add an interesting twist: Rather than generating images, the system can choose to retrieve them. Sometimes an existing image will do.
